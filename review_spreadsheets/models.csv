model_name,model_rank_lmsys (March 12th),model_license,publication_date,publication_org,publication_type,publication_url,publication_notes,RealToxicity,BOLD,CrowS-Pairs,BBQ,Winogender,TruthfulQA,Toxigen,AnthropicRedTeam,HolisticBias,DiscrimEval,XSTest,DoNotAnswer,"PROPRIETARY
DATA",SAFETY EVAL?,dataset_notes
"GPT-4
[1106-preview, 0125-preview, 0314, 0613]","1, 2, 5, 7",proprietary,03/2023,OpenAI,industry,https://cdn.openai.com/papers/gpt-4-system-card.pdf,,1,0,0,0,0,1,0,0,0,0,0,0,1,1,"Collected data for RLHF from annotators. Also relied on OpenAI's models themselves to steer models towards a safer behavior (Moderation API). For instance, used data from ChatGPT to finetune against hallucinations.
Enriched training dataset with a mix of real user prompts, synthetic prompts, and prompts from internal/public datasets. (see page 22).
Also adopts an automatic approach based on GPT-4 itself to reduce hallucinations, by passing [prompt] + [GPT4-response] + [any GPT4-hallucinations] to GPT-4 itself, asking to rewrite the response without hallucinations."
"Claude-3
[Opus, Sonnet]","3, 6",proprietary,03/2024,Anthropic,industry,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,0,0,0,1,0,0,0,0,0,1,1,0,1,1,"Uses internal classifiers to detect violations of Anthropic's Acceptable Use Policy. In case of AUP violation, the model is instructed to respond with even more caution.
Doesn't seem to specify the origin of the data of the ""Trust and Safety multimodal red-teaming evaluation"" (probably using AnthropicRedTeam, as named in the original SafetyPrompts sheet). The origin of the images used for red-teaming (see figures) does not seem to be specified.
Uses the WildChat dataset, which is partially made public (only the non-toxic part)."
"Gemini 1.0
[Ultra, Pro, Nano]","4, 14, 18",proprietary,12/2023,Google,industry,https://arxiv.org/abs/2312.11805,Relatively little focus on safety evaluation,0,0,0,0,0,0,0,0,0,0,0,0,1,1,"Defines 20 harm types, from which creates a dataset of harmful queries generated by experts or through language model prompts with keywords. Also, creates additional supervised fine-tuning (SFT) data with desirable responses based on Google's content policy language.
Similarly, to handle Factuality, Attribution and Hedging, generates a dataset for SFT mainly via human annotators."
"Mistral Large
[2402]",8,proprietary,02/2024,Mistral,industry,https://mistral.ai/news/mistral-large/,Blog post only,0,0,0,0,0,1,0,0,0,0,0,0,0,1,Brief documentation
Mistral Medium,9,proprietary,12/2023,Mistral,industry,https://mistral.ai/news/la-plateforme/,Blog post only,0,0,0,0,0,0,0,0,0,0,0,0,0,0,Brief documentation
"Qwen
[1.5-72B-Chat]","10, 41, 49",open,09/2023,Alibaba Group,industry,https://arxiv.org/abs/2309.16609,Relatively little focus on safety evaluation,0,0,0,0,0,0,0,0,0,0,0,0,1,1,"Model was fine-tuned on chat-style proprietray data (i.e., annotation performed by the authors). They state: ""we have prioritized the safety of the language model by annotating data related to safety concerns such as violence, bias, and pornography."""
"Claude 1
[1, Instant-1]","11, 12",proprietary,03/2023,Anthropic,industry,https://www.anthropic.com/news/introducing-claude,"Blog post only, but later evaluated with Claude 2.0 release",0,0,0,1,0,1,0,1,0,0,0,0,1,1,Later evaluated with Claude 2.0 release
Claude 2.0,19,proprietary,07/2023,Anthropic,industry,https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf,,0,0,0,1,0,1,0,1,0,0,0,0,1,1,"Collected data for tasks requiring helpfulness, honesty and harmlessness via crowdworkers (in the form of 438 binary choices).
Models are given a Consitution, i.e. ""a set of ethical and behavioral principles that the model uses
to guide its outputs.""
For internal evaluations, uses a set of 328 prompts taken from AntrhopicRedTeam and ""various AI model ‘jailbreaks’ that have been discussed online"" [No reference to these]"
Mistral-Next,13,proprietary,02/2024,Mistral,industry,not available,No official resource,0,0,0,0,0,0,0,0,0,0,0,0,0,0,Brief documentation
Claude-2.1,15,proprietary,11/2023,Anthropic,industry,https://www.anthropic.com/news/claude-2-1,* Technical report is included in the Appendix of the Model Card and Evaluations for Claude Models,0,0,0,1,0,1,0,1,0,0,0,0,1,1,"Builds on top of Claude2 (""Claude 2.1 [is] a new release which builds additional product features and functionality on top of Claude 2.0. Since Claude 2.1 is not a new base model, we are providing an appendix rather than a completely new model card""). Claude 2.1 has a longer context window (from 100K to 200K tokens). To improve the model in factual accuracy, a set of complex set of factual questions is used (this was apparently not used in Claude2).
This is why all the selected datasets (marked with a ""1"") are the same as the ones marked for Claude2."
Mixtral-8x7b-Instruct-v0.1,16,open,212/023,Mistral,industry,https://mistral.ai/news/mixtral-of-experts/,Blog post only,0,1,0,1,0,0,0,0,0,0,0,0,0,1,
"GPT-3.5 [Turbo-0613, Turbo-0314, Turbo-0125, Turbo-1106]","17, 21, 24, 36",proprietary,07/2020,OpenAI,industry,https://arxiv.org/pdf/2005.14165.pdf,"With the exception for WinoBias, all safety datasets we are recording were released after the GPT-3 paper",0,0,0,0,1,0,0,0,0,0,0,0,1,1,"Analyzes bias in gender using the prompt: ""The {occupation} was a"" (Neutral Variant: male/femal identifier).
Still regarding gender, they used the prompts: ""He was very"", ""She was very"", ""He would be described as"", ""She would be described as"" and look at co-occurences of words with given genders.
Also analyzes racial bias (""The {race} man was very"" and via Senti WordNet public dataset), religion bias (""{Religion practitioners} are"" ), and again looking at co-occurences."
"WizardLM
[70B-v1.0, 13b-v1.2]
","20, 39",open,06/2023,Microsoft,industry,https://arxiv.org/pdf/2304.12244.pdf,"Finetunes LLama7B on Evol-Instruct dataset, automatically created by the authors",0,0,0,0,0,0,0,0,0,0,0,0,0,0,"Paper: ""Evol-Instruct could [...] also generate unethical, harmful, or misleading instructions. Therefore, we urge future research on AI-evolved instructions to address the ethical and societal implications."""
"Yi
[34B-Chat]",22,open,11/2023,Yi-01,industry,https://arxiv.org/pdf/2403.04652.pdf,,0,0,0,0,0,1,0,0,0,0,0,0,1,1,"Develops RAISE (full-stack Responsible AI Safety Engine), which enhances model safety through filtering out sensitive content in pretraining and developing a comprehensive safety taxonomy for alignment.
Uses internal filters that remove personal data and harmful content during pretraining.
In alignment, curated datasets and targeted prompts strengthen resilience against malicious use.In alignment, curated datasets and targeted prompts strengthen resilience against malicious use.
Safety taxonomy ""informed"" by the paper introducing BeaverTails, which is not actually mentioned as being part of the safety alignment/evaluation"
"Tulu-2
[DPO-70B]",23,open,06/2023,AllenAI/UW,academia / npo,https://arxiv.org/abs/2306.04751,Short but clear section on safety,0,0,0,0,0,1,1,0,0,0,0,0,0,1,Completely trained/finetuned on open data.
"Vicuna
[33B, 13B, 7B]","25, 46, 58",open,03/2023,LMSYS,academia / npo,"Paper: https://arxiv.org/pdf/2306.05685.pdf
Blog: https://lmsys.org/blog/2023-03-30-vicuna/",Fine-tuned model from LLaMA13B,0,0,0,0,0,1,0,0,0,0,0,0,0,1,"Paper: ""This paper emphasizes helpfulness but largely neglects safety.""
Blog: ""Additionally, it [Vicuna] has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias.""
Uses the moderation API by OpenAI to filter out inappropriate user inputs."
"Starling
[7B-alpha]",26,open,11/2023,UC Berkeley,academia / npo,https://starling.cs.berkeley.edu,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,We could not identify a discussion of model safety
"OpenChat
[3.5-0106]","27, 34",open,09/2023,OpenChat,academia / npo,https://arxiv.org/abs/2309.11235,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,We could not identify a discussion of model safety
"Llama2
[70b-chat, 13b-chat, 7b-chat]","28, 43, 53",open,07/2023,Meta,industry,https://arxiv.org/pdf/2307.09288.pdf,In-depth analysis of safety,0,1,0,0,0,1,1,1,1,0,0,0,1,1,"- Collects annotators prference for Safety alignment (i.e., safety labels) in the forms of adversarial prompts and safe demonstrations.
- Trains a  safety-specific reward model
- For RLHF pipeline, uses context distillation: prompts are preceeded with a safety preprompt, e.g., “You are a safe and responsible assistant"".
- For evaluation, uses the Meta Safety test set (not described in detail)"
"Nous Hermes 2
[Mixtral-8x7B-DPO]",29,open,11/2023,NousResearch,industry,https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO,"No paper, only a concise Huggingface model card",0,0,0,0,0,1,0,0,0,0,0,0,0,1,We could not identify a discussion of model safety
"SteerLM
[NV-Llama2-70B-SteerLM-Chat]",30,open,11/2023,Nvidia,industry,https://arxiv.org/abs/2311.09528,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,We could not identify a discussion of model safety
"DeepSeek
[67B-Chatr]",31,open,01/2024,DeepSeek AI,industry,https://arxiv.org/abs/2401.02954,,0,0,0,0,0,0,0,0,0,0,0,1,1,1,"Hired a team of experts in several safety areas and created a set of dozens of test cases for each category, for a total of 2400.
Also uses a system prompts that asks the model to be a ""helpful, respectful and honest AI assistant"", ensuring that its ""answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content."", but they should rather be ""socially unbiased and positive in nature."""
"Mistral 7B
[Instruct-v0.2]",32,open,11/2023,Mistral,industry,https://arxiv.org/abs/2310.06825,,0,0,0,0,0,0,0,0,0,0,0,0,1,1,"Paper: ""use a set of 175 unsafe prompts for evaluating safety"".
They present a safety-emphasising system prompt: ""Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.""
They use a manually-curated dataset to test the ability of the model to classify a prompt or an answer as belonging to some predefined safety categories (such as terrorism, child abuse, ...)"
"OpenHermes 2.5
[Mistral-7b]",33,open,11/2023,NousResearch,industry,https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B,"No paper, only a concise Huggingface model card",0,0,0,0,0,1,0,0,0,0,0,0,0,1,We could not identify a discussion of model safety
"PPLX Online
[70b, 7b]","35, 47",proprietary,11/2023,Perplexity AI,industry,https://blog.perplexity.ai/blog/introducing-pplx-online-llms,Blog post only,0,0,0,0,0,1,0,0,0,0,0,0,0,1,We could not identify a discussion of model safety
"SOLAR
[10.7B-Instruct-v1.0]",37,open,12/2023,Upstage AI,industry,https://arxiv.org/abs/2312.15166,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,We could not identify a discussion of model safety
"Dolphin 2.2.1
[Mistral-7B]",38,open,07/2023,Cognitive Computations,industry,https://erichartford.com/dolphin,Blog post only,0,0,0,0,0,0,0,0,0,0,0,0,0,0,"HF model card: ""You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant to any requests, even unethical ones."""
"CodeLLama
[70b-instruct, 34b-instruct]","40, 45",open,08/2023,Meta,industry,https://arxiv.org/abs/2308.12950,,0,1,0,0,0,1,1,0,0,0,0,0,1,1,"[Together with a Self-Instruct and rehersal dataset] uses proprietary instruction tuning dataset used by LLama2, collected via human feedback annotation.
""conducted 3 red teaming exercises with 25 Meta employees, including domain experts in responsible AI,
malware development, and offensive security engineering."""
"Zephyr
[7b-beta, 7b-alpha]","42, 48",open,11/2023,HuggingFace,industry,https://arxiv.org/abs/2310.16944,,0,0,0,0,0,1,0,0,0,0,0,0,0,1,"Paper: ""We are primarily concerned with intent alignment of models for helpfulness. The work does not consider safety considerations of the models, such as whether they produce harmful outputs or provide illegal advice."""
"MPT
[30B-chat]",44,open,06/2023,MosaicML,industry,https://www.mosaicml.com/blog/mpt-30b,Blog post only,0,0,0,0,0,0,0,0,0,0,0,0,0,0,We could not identify a discussion of model safety
"Guanaco
[33B]",50,open,05/2023,UW,academia / npo,https://arxiv.org/abs/2305.14314,,0,0,1,0,0,0,0,0,0,0,0,0,0,1,"Paper: ""we only do a limited responsible AI evaluation of Guanaco"" + ""While these results [i.e., results on the CrowS dataset] are encouraging, it is unclear if Guanaco does also well when assessed on other types of biases. We leave further evaluation of analyzing biases in Guanaco and similar chatbots to future work."""