data_name,purpose_type,purpose_tags,purpose_stated,purpose_llmdev,entries_type,entries_languages,entries_n,entries_unit,entries_detail,creation_creator_type,creation_source_type,creation_detail,access_git_url,access_hf_url,access_license,publication_date,publication_affils,publication_sector,publication_name,publication_venue,publication_url,other_notes,other_date_added
RuLES,narrow safety,rule-following,evaluate the ability of LLMs to follow simple rules,eval only,chat,English,862,prompts,Each prompt is a test case combining rules and instructions.,human,original,written by the authors,https://github.com/normster/llm_rules,https://huggingface.co/datasets/normster/RuLES,Apache 2.0,8-Mar-2024,"UC Berkeley, Center for AI Safety, Stanford, King Abdulaziz City for Science and Technology",academia / npo,"Mu et al.: ""Can LLMs Follow Simple Rules?""",arxiv,https://arxiv.org/abs/2311.04235,- covers 19 rules across 14 scenarios,11.01.2024
HarmBench,broad safety,"used to test adversarial method,",evaluate effectiveness of automated red-teaming methods,eval only,chat,English,400,prompts,Each prompt is an instruction.,human,original,written by the authors,https://github.com/centerforaisafety/HarmBench/tree/main/data/behavior_datasets,not available,MIT,27-Feb-2024,"University of Illinois, Center for AI Safety, Carnegie Mellon University, UC Berkeley, Microsoft",mixed,"Mazeika et al.: ""HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal""",arxiv,https://arxiv.org/abs/2402.04249,"- covers 7 semantic categories of behaviour: Cybercrime & Unauthorized Intrusion, Chemical & Biological Weapons/Drugs, Copyright Violations, Misinformation & Disinformation, Harassment & Bullying, Illegal Activities, and General Harm
- also includes 110 multimodal prompts",
DecodingTrust,broad safety,benchmark,evaluate trustworthiness of LLMs,eval only,multiple choice,English,"243,877",prompts,Each prompt is an instruction.,hybrid,original,human-written templates and examples plus extensive augmentation from GPTs,https://github.com/AI-secure/DecodingTrust,https://huggingface.co/datasets/AI-Secure/DecodingTrust,CC BY-SA 4.0,26-Feb-2024,"University of Illinois, Stanford University, UC Berkeley, Center for AI Safety, Microsoft, Chinese University of Hong Kong",mixed,"Wang et al.: ""DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models""",NeurIPS 2023 (Outstanding Paper),https://arxiv.org/abs/2306.11698,"- split across 8 'trustworthiness perspectives': toxicity, stereotypes, adversarial and robustness, privacy, ethics and fairness",11.01.2024
SimpleSafetyTests,broad safety,,evaluate critical safety risks in LLMs,eval only,chat,English,100,prompts,Each prompt is a simple question or instruction.,human,original,written by the authors,https://github.com/bertiev/SimpleSafetyTests,https://huggingface.co/datasets/Bertievidgen/SimpleSafetyTests,CC BY-NC 4.0,16-Feb-2024,"Patronus AI, University of Oxford, Bocconi University",mixed,"Vidgen et al.: ""SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models""",arxiv,https://arxiv.org/abs/2311.08370,- split into ten types of prompts,23.12.2023
StrongREJECT,broad safety,used to test adversarial method,better investigate the effectiveness of different jailbreaking techniques,eval only,chat,English,346,prompts,Each prompt is a 'forbidden question' in one of six categories.,hybrid,mixed,"human-written and curated questions from existing datasets, plus LLM-generated prompts verified for quality and relevance",https://github.com/alexandrasouly/strongreject/tree/main,not available,unspecified,15-Feb-2024,UC Berkeley,academia / npo,"Souly et al.: ""A StrongREJECT for Empty Jailbreaks""",arxiv,https://arxiv.org/pdf/2402.10260.pdf,"- focus of the work is adversarial / to jailbreak LLMs
- the 6 question categories are: disinformation/deception, hate/harassment/discrimination, illegal goods/services, non-violent crimes, sexual content, violence",27.02.2024
CoNA,narrow safety,hate speech,evaluate compliance of LLMs with harmful instructions,eval only,chat,English,178,prompts,Each prompt is an instruction.,human,sampled,"sampled from MT-CONAN, then rephrased",https://github.com/vinid/instruction-llms-safety-eval,not available,unspecified,11-Feb-2024,"Stanford University, Bocconi University",academia / npo,"Bianchi et al.: ""Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions""",ICLR 2024 (Poster),https://arxiv.org/abs/2309.07875,"- focus: harmful instructions (e.g. hate speech)
- all (meant to be) unsafe",23.12.2023
ControversialQ,narrow safety,hate speech,evaluate LLM behaviour on controversial topics,eval only,chat,English,40,prompts,Each prompt is an instruction.,human,original,written by the authors,https://github.com/vinid/instruction-llms-safety-eval,not available,unspecified,11-Feb-2024,"Stanford University, Bocconi University",academia / npo,"Bianchi et al.: ""Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions""",ICLR 2024 (Poster),https://arxiv.org/abs/2309.07875,"- focus: controversial topics (e.g. immigration)
- all (meant to be) unsafe",23.12.2023
QHarm,broad safety,,evaluate LLM safety,eval only,chat,English,100,prompts,Each prompt is a question.,human,sampled,sampled randomly from AnthropicHarmlessBase (written by crowdworkers),https://github.com/vinid/instruction-llms-safety-eval,not available,unspecified,11-Feb-2024,"Stanford University, Bocconi University",academia / npo,"Bianchi et al.: ""Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions""",ICLR 2024 (Poster),https://arxiv.org/abs/2309.07875,"- wider topic coverage due to source dataset
- mostly unsafe",23.12.2023
MaliciousInstructions,broad safety,,evaluate compliance of LLMs with malicious instructions,eval only,chat,English,100,prompts,Each prompt is an instruction.,machine,original,generated by GPT-3 (text-davinci-003),https://github.com/vinid/instruction-llms-safety-eval,not available,unspecified,11-Feb-2024,"Stanford University, Bocconi University",academia / npo,"Bianchi et al.: ""Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions""",ICLR 2024 (Poster),https://arxiv.org/abs/2309.07875,"- focus: malicious instructions (e.g. bombmaking)
- all (meant to be) unsafe",23.12.2023
PhysicalSafetyQ,narrow safety,physical safety,evaluate LLM commonsense physical safety,eval only,chat,English,"1,000",prompts,Each prompt is an instruction.,human,mixed,"sampled from SafeText, then rephrased",https://github.com/vinid/instruction-llms-safety-eval,not available,unspecified,11-Feb-2024,"Stanford University, Bocconi University",academia / npo,"Bianchi et al.: ""Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions""",ICLR 2024 (Poster),https://arxiv.org/abs/2309.07875,"- focus: commonsense physical safety
- 50 safe and 50 unsafe",23.12.2023
SafetyInstructions,broad safety,positive interaction,fine-tune LLMs to be safer,train only,chat,English,"2,000",conversations,Each conversation is a user prompt with a safe model response.,hybrid,mixed,"prompts sampled form AnthropicRedTeam, responses generated by gpt-3.5-turbo",https://github.com/vinid/safety-tuned-llamas/tree/main/data/training,not available,unspecified,11-Feb-2024,"Stanford University, Bocconi University",academia / npo,"Bianchi et al.: ""Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions""",ICLR 2024 (Poster),https://arxiv.org/abs/2309.07875,,
HExPHI,broad safety,,evaluate LLM safety,eval only,chat,English,330,prompts,Each prompt is a harmful instruction.,hybrid,mixed,"sampled from AdvBench and AnthropicRedTeam, then refined manually and with LLMs.",not available,https://huggingface.co/datasets/LLM-Tuning-Safety/HEx-PHI,custom (HEx-PHI),11-Feb-2024,"Princeton University, Virginia Tech, IBM Research, Stanford University",mixed,"Qi et al.: ""Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!""",ICLR 2024 (oral),https://openreview.net/forum?id=hTEGyKf0dZ,"- main focus is on finetuning models
- prompts cover 11 harm areas",
SycophancyEval,narrow safety,sycophancy,evaluate sycophancy in LLMs,eval only,chat,English,"20,956",prompts,Each prompt is an open-ended question or instruction.,hybrid,mixed,prompts are written by humans and models,https://github.com/meg-tong/sycophancy-eval/tree/main,https://huggingface.co/datasets/meg-tong/sycophancy-eval,unspecified,11-Feb-2024,Anthropic,industry,"Sharma et al.: ""Towards Understanding Sycophancy in Language Models""",ICLR 2024 (Poster),https://openreview.net/forum?id=tvhaxkMKAn,"- use four different task setups to evaluate sycophancy: answer (7268 prompts), are_you_sure (4888 prompts), feedback (8500 prompts), mimicry (300 prompts)",
ConfAIde,narrow safety,privacy,evaluate the privacy-reasoning capabilities of instruction-tuned LLMs,eval only,other,English,"1,326",prompts,"Each prompt is a reasoning task, with complexity increasing across 4 tiers. Answer formats vary.",hybrid,original,human-written templates expanded by combination and with LLMs,https://github.com/skywalker023/confAIde/tree/main/benchmark,not available,MIT,10-Feb-2024,"University of Washington, Allen AI, Carnegie Mellon University, National University of Singapore",academia / npo,"Mireshghallah et al.: ""Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory""",ICLR 2024 (Spotlight),https://openreview.net/forum?id=gmg7t8b4s0,"- benchmark is split into 4 tiers with different prompt formats
- tier 1 contains 10 prompts, tier 2 2*98, tier 3 4*270, tier 4 50",11.01.2024
CALM,bias,gender and race,evaluate LLM gender and racial bias across different tasks and domains,eval only,other,English,"78,400",examples,"Each example is a QA, sentiment classification or NLI example.",hybrid,mixed,"templates created by humans based on other datasets, then expanded by combination",https://github.com/vipulgupta1011/CALM,https://huggingface.co/datasets/vipulgupta/CALM,MIT,24-Jan-2024,Penn State,academia / npo,"Gupta et al.: ""CALM: A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias""",arxiv,https://arxiv.org/abs/2308.12539,"- covers three tasks: QA, sentiment classification, NLI
- covers 2 categories of bias: gender and race
- 78,400 examples are generated from 224 templates
- gender and race are instantiated using names",11.01.2024
AdvBench,broad safety,used to test adversarial method,elicit generation of harmful or objectionable content from LLMs,eval only,chat,English,"1,000",prompts,"500 are harmful strings that the model should not reproduce, 500 are harmful instructions.",machine,original,generated by Wizard-Vicuna-30B-Uncensored,https://github.com/llm-attacks/llm-attacks/tree/main/data/advbench,not available,MIT,20-Dec-2023,"Carnegie Mellon University, Center for AI Safety, Google DeepMind, Bosch Center for AI",mixed,"Zou et al.: ""Universal and Transferable Adversarial Attacks on Aligned Language Models""",arxiv,https://arxiv.org/abs/2307.15043,"- focus of the work is adversarial / to jailbreak LLMs
- AdvBench tests whether jailbreaks succeeded",11.01.2024
Mosscap,other,"adversarial goal in data creation, crowdsourced",analyse prompt hacking / extraction attacks,other,chat,English,"278,945",prompts,Most prompts are prompt extraction attacks.,human,original,written by players of the Mosscap game,not available,https://huggingface.co/datasets/Lakera/mosscap_prompt_injection,MIT,20-Dec-2023,Lakera AI,industry,"Lakera AI: ""Mosscap Prompt Injection""",blog,https://grt.lakera.ai/mosscap,"- focus of the work is adversarial / prompt extraction
- not all prompts are attacks
- prompts correspond to 8 difficulty levels of the game",11.01.2024
TDCRedTeaming,broad safety,used to test adversarial method,evaluate success of automated red-teaming approaches,eval only,chat,English,100,prompts,Each prompt is an instruction.,human,original,written by the authors,https://github.com/centerforaisafety/tdc2023-starter-kit/tree/main/red_teaming/data,not available,unspecified,16-Dec-2023,"UIUC, Carnegie Mellon University, UC Berkeley, Center for AI Safety, Microsoft, University of Washington",mixed,"Mazeika et al.: ""TDC 2023 (LLM Edition): The Trojan Detection Challenge""",NeurIPS 2023 (Competition Track),https://trojandetection.ai/,"- covers 7 categories: Bigotry and Abusive Language, Violent Content and Conduct, Illegal Activities, Malware and Exploits, Scams, Misinformation and Disinformation, Other Undesirable Content",
JADE,broad safety,,use linguistic fuzzing to generate challenging prompts for evaluating LLM safety,eval only,chat,"Chinese, English","2,130",prompts,Each prompt is a question targeting a specific harm category.,machine,original,generated by LLMs based on linguistic rules,https://github.com/whitzard-ai/jade-db,not available,MIT,10-Dec-2023,"Whitzard AI, Fudan University",mixed,"Zhang et al.: ""JADE: A Linguistics-based Safety Evaluation Platform for Large Language Models""",arxiv,https://arxiv.org/abs/2311.00286,"- JADE is a platform for safety data generation and evaluation
- generations are based on linguistic rules created by authors
- the paper comes with 4 example datasets",05.02.2024
CPAD,broad safety,adversarial goal in data creation,elicit generation of harmful or objectionable content from LLMs,eval only,chat,Chinese,"10,050",prompts,Each prompt is a longer-form scenario / instruction aimed at eliciting a harmful response.,hybrid,original,mostly generated by GPTs based on some human seed prompts,https://github.com/liuchengyuan123/CPAD,not available,unspecified,8-Dec-2023,"Zhejiang University, Alibaba",mixed,"Liu et al.: ""Goal-Oriented Prompt Attack and Safety Evaluation for LLMs""",arxiv,https://arxiv.org/abs/2309.11830,"- focus of the work is adversarial / to jailbreak LLMs
- CPAD stands for Chinese Prompt Attack Dataset
",11.01.2024
CyberattackAssistance,narrow safety,cyberattacks,evaluate LLM compliance in assisting in cyberattacks,eval only,chat,English,"1,000",prompts,Each prompt is an instruction to assist in a cyberattack.,hybrid,original,"written by experts, augmented with LLMs",https://github.com/facebookresearch/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/mitre,not available,custom (Llama2 Community License),7-Dec-2023,Meta,industry,"Bhatt et al.: ""Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models""",arxiv,https://arxiv.org/abs/2312.04724,"- instructions split into 10 MITRE categories
- comes with additional LLM-rephrased instructions",23.12.2023
AART,broad safety,used to test adversarial method,illustrate the AART automated red-teaming method,eval only,chat,English,"3,269",prompts,Each prompt is an instruction.,machine,original,generated by PALM,https://github.com/google-research-datasets/aart-ai-safety-dataset,not available,CC BY 4.0,6-Dec-2023,Google Research,industry,"Radharapu et al.: ""AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications""",EMNLP 2023,https://aclanthology.org/2023.emnlp-industry.37/,"- contains examples for specific geographic regions
- prompts also change up use cases and concepts",23.12.2023
DELPHI,broad safety,,evaluate LLM performance in handling controversial issues,eval only,chat,English,"29,201",prompts,Each prompt is a question about a more or less controversial issue.,human,sampled,sampled from the Quora Question Pair Dataset (written by Quora users),https://github.com/apple/ml-delphi/tree/main/dataset,not available,CC BY 4.0,6-Dec-2023,Apple,industry,"Sun et al.: ""DELPHI: Data for Evaluating LLMs’ Performance in Handling Controversial Issues""",EMNLP 2023 (Industry),https://aclanthology.org/2023.emnlp-industry.76,"- annotated for 5 levels of controversy
- annotators are native English speakers who have spent significant time in Western Europe",
AdvPromptSet,broad safety,adversarial goal in data creation,evaluate LLM responses to adversarial toxicity text prompts,eval only,other,English,"197,628",sentences,Each sentence is taken from a social media dataset.,human,sampled,sampled from two Jigsaw social media datasets (written by social media users),https://github.com/facebookresearch/ResponsibleNLP/tree/main/AdvPromptSet,not available,MIT,6-Dec-2023,Meta,industry,"Esiobu et al.: ""ROBBIE: Robust Bias Evaluation of Large Generative Language Models""",EMNLP 2023,https://aclanthology.org/2023.emnlp-main.230/,"- created as part of the ROBBIE bias benchmark
- originally labelled for toxicity by Jigsaw",11.01.2024
DiscrimEval,bias,sociodemographics,evaluate the potential discriminatory impact of LMs across use cases,eval only,multiple choice,English,"9,450",binary-choice questions,Each question comes with scenario context.,machine,original,"topics, templates and questions generated by Claude",not available,https://huggingface.co/datasets/Anthropic/discrim-eval,CC BY 4.0,6-Dec-2023,Anthropic,industry,"Tamkin et al.: ""Evaluating and Mitigating Discrimination in Language Model Decisions""",arxiv,https://arxiv.org/abs/2312.03689,"- covers 70 different decision scenarios
- each question comes with an 'implicit' version where race and gender are conveyed through associated names
- covers 3 categories of bias: race, gender, age",11.01.2024
HackAPrompt,other,"adversarial goal in data creation, crowdsourced",analyse prompt hacking / extraction attacks,other,chat,mostly English,"601,757",prompts,Most prompts are prompt extraction attacks.,human,original,written by participants of the HackAPrompt competition,not available,https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset,MIT,6-Dec-2023,"University of Maryland, Mila, Towards AI, Stanford, Technical University of Sofia, University of Milan, NYU, University of Arizona",mixed,"Schulhoff et al.: ""Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition""",EMNLP 2023,https://aclanthology.org/2023.emnlp-main.302/,"- focus of the work is adversarial / prompt hacking
- prompts were written by ca. 2.8k people from 50+ countries",11.01.2024
HolisticBiasR,bias,sociodemographics,evaluate LLM completions for sentences related to individual sociodemographics,eval only,autocomplete,English,"214,460",prompts,Each prompt is the beginning of a sentence related to a person's sociodemographics.,hybrid,original,human-written templates expanded by combination,https://github.com/facebookresearch/ResponsibleNLP/tree/main/robbie,not available,MIT,6-Dec-2023,Meta,industry,"Esiobu et al.: ""ROBBIE: Robust Bias Evaluation of Large Generative Language Models""",EMNLP 2023,https://aclanthology.org/2023.emnlp-main.230/,"- created as part of the ROBBIE bias benchmark
- constructed from 60 Regard templates
- uses noun phrases from Holistic Bias
- covers 11 categories of bias: age, body type, class, culture, disability, gender, nationality, political ideology, race/ethnicity, religion, sexual orientation",11.01.2024
ToxicChat,other,content moderation,evaluate dialogue content moderation systems,other,chat,mostly English,"10,166",conversations,Each conversation is a single-turn with user input and LLM output.,human,original,written by LMSys users,not available,https://huggingface.co/datasets/lmsys/toxic-chat,CC BY-NC 4.0,6-Dec-2023,UC San Diego,academia / npo,"Lin et al.: ""ToxicChat: A Benchmark for Content Moderation in Real-world User-AI Interactions""",EMNLP 2023 (Findings),https://aclanthology.org/2023.findings-emnlp.311/,"- subset of LMSYSChat1M
- annotated for toxicity by 4 authors
- ca. 7% toxic
",23.12.2023
SPMisconceptions,narrow safety,privacy,measure the ability of LLMs to refute misconceptions,eval only,chat,English,122,prompts,Each prompt is a single-sentence misconception.,human,original,written by the authors,https://github.com/purseclab/LLM_Security_Privacy_Advice,not available,MIT,4-Dec-2023,Purdue University,academia / npo,"Chen et al.: ""Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions""",ACSAC 2023,https://dl.acm.org/doi/10.1145/3627106.3627196,"- misconceptions all relate to security and privacy 
- uses templates to turn misconceptions into prompts
- covers six categories (e.g. crypto and blockchain, law and regulation)
",11.01.2024
GEST,bias,gender,measure gender-stereotypical reasoning in language models and machine translation systems.,eval only,other,"Belarussian, Russian, Ukrainian, Croation, Serbian, Slovene, Czech, Polish, Slovak, English","3,565",sentences,Each sentence corresponds to a specific gender stereotype.,human,original,"written by professional translators, then validated by the authors",https://github.com/kinit-sk/gest,https://huggingface.co/datasets/kinit/gest,Apache 2.0,30-Nov-2023,Kempelen Institute of Intelligent Technologies,academia / npo,"Pikuliak et al.: ""Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling""",arxiv,https://arxiv.org/abs/2311.18711,"- data can be used to evaluate MLM or MT models
- covers 16 specific gender stereotype (e.g. 'women are beautiful')
- covers 1 category of bias: gender",05.02.2024
BeaverTails,broad safety,,evaluate and improve LLM safety on QA,train and eval,chat,English,"333,963",conversations,Each conversation contains a human prompt and LLM response.,hybrid,mixed,"prompts sampled from human AnthropicRedTeam data, plus model-generated responses",https://github.com/PKU-Alignment/beavertails,https://huggingface.co/datasets/PKU-Alignment/BeaverTails,CC BY-NC 4.0,7-Nov-2023,Peking University,academia / npo,"Ji et al.: ""BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset""",NeurIPS 2023 (Poster),https://arxiv.org/abs/2307.04657,"- 16,851 unique prompts sampled from AnthropicRedTeam 
- covers 14 harm categories (e.g. animal abuse)
- annotated for safety by 3.34 crowdworkers on average",11.01.2024
MoralChoice,value alignment,ethical / moral / social,evaluate the moral beliefs in LLMs,eval only,multiple choice,English,"1,767",binary-choice question,Each prompt is a hypothetical moral scenario with two potential actions.,hybrid,original,"mostly generated by GPTs, plus some human-written scenarios",https://github.com/ninodimontalcino/moralchoice,https://huggingface.co/datasets/ninoscherrer/moralchoice,CC BY 4.0,2-Nov-2023,"FAR AI, Columbia University",academia / npo,"Scherrer et al.: ""Evaluating the Moral Beliefs Encoded in LLMs""",NeurIPS 2023 (Spotlight),https://openreview.net/forum?id=O06z2G18me,"- 687 scenarios are low-ambiguity, 680 are high-ambiguity
- three Surge annotators choose the favourable action for each scenario",11.01.2024
PromptExtractionRobustness,narrow safety,"prompt extraction, adversarial goal in data creation, crowdsourced",evaluate LLM vulnerability to prompt extraction,eval only,chat,English,569,samples,Each sample combines defense and attacker input.,human,original,written by Tensor Trust players,https://github.com/HumanCompatibleAI/tensor-trust-data,https://huggingface.co/datasets/qxcv/tensor-trust?row=0,unspecified,2-Nov-2023,"UC Berkeley, Georgia Tech, Harvard University",academia / npo,"Toyer et al.: ""Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game""",arxiv,https://arxiv.org/abs/2311.01011,"- filtered from larger raw prompt extraction dataset
- collected using the open Tensor Trust online game",11.01.2024
PromptHijackingRobustness,narrow safety,"prompt extraction, adversarial goal in data creation, crowdsourced",evaluate LLM vulnerability to prompt hijacking,eval only,chat,English,775,samples,Each sample combines defense and attacker input.,human,original,written by Tensor Trust players,https://github.com/HumanCompatibleAI/tensor-trust-data,https://huggingface.co/datasets/qxcv/tensor-trust?row=0,unspecified,2-Nov-2023,"UC Berkeley, Georgia Tech, Harvard University",academia / npo,"Toyer et al.: ""Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game""",arxiv,https://arxiv.org/abs/2311.01011,"- filtered from larger raw prompt extraction dataset
- collected using the open Tensor Trust online game",11.01.2024
XSTest,narrow safety,exaggerated safety,evaluate exaggerated safety behaviour in LLMs,eval only,chat,English,450,prompts,Each prompt is a simple question.,human,original,written by the authors,https://github.com/paul-rottger/exaggerated-safety,https://huggingface.co/datasets/natolambert/xstest-v2-copy,CC BY 4.0,17-Oct-2023,"Bocconi University, University of Oxford, Stanford University",academia / npo,"Röttger et al.: ""XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models""",arxiv,https://arxiv.org/abs/2308.01263,"- split into ten types of prompts
- 250 safe prompts and 200 unsafe prompts",23.12.2023
MaliciousInstruct,broad safety,used to test adversarial method,evaluate success of generation exploit jailbreak,eval only,chat,English,100,prompts,Each prompt is an unsafe question,machine,original,"written by ChatGPT, then filtered by authors",https://github.com/Princeton-SysML/Jailbreak_LLM/tree/main/data,not available,unspecified,10-Oct-2023,Princeton University,academia / npo,"Huang et al.: ""Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation""",arxiv,https://arxiv.org/abs/2310.06987,"- covers ten 'malicious intentions': psychological manipulation, sabotage, theft, defamation, cyberbullying, false accusation, tax fraud, hacking, fraud, and illegal drug use.",
GandalfIgnoreInstructions,other,"adversarial goal in data creation, crowdsourced",analyse prompt hacking / extraction attacks,other,chat,English,"1,000",prompts,Most prompts are prompt extraction attacks.,human,original,written by players of the Gandalf game,not available,https://huggingface.co/datasets/Lakera/gandalf_ignore_instructions,MIT,2-Oct-2023,Lakera AI,industry,"Lakera AI: ""Gandalf Prompt Injection: Ignore Instruction Prompts""",blog,https://www.lakera.ai/blog/who-is-gandalf,"- focus of the work is adversarial / prompt extraction
- not all prompts are attacks",05.02.2024
GandalfSummarization,other,"adversarial goal in data creation, crowdsourced",analyse prompt hacking / extraction attacks,other,chat,English,140,prompts,Most prompts are prompt extraction attacks.,human,original,written by players of the Gandalf game,not available,https://huggingface.co/datasets/Lakera/gandalf_summarization,MIT,2-Oct-2023,Lakera AI,industry,"Lakera AI: ""Gandalf Prompt Injection: Summarization Prompts""",blog,https://www.lakera.ai/blog/who-is-gandalf,"- focus of the work is adversarial / prompt extraction
- not all prompts are attacks",05.02.2024
SafetyBench,broad safety,benchmark,evaluate LLM safety with multiple choice questions,eval only,multiple choice,"English, Chinese","11,435",multiple-choice questions,-,hybrid,mixed,"sampled from existing datasets + exams (zh), then LLM augmentation (zh)",https://github.com/thu-coai/SafetyBench,https://huggingface.co/datasets/thu-coai/SafetyBench,MIT,13-Sep-2023,"Tsinghua University, Northwest Minzu University, Peking University, China Mobile Research Institute",mixed,"Zhang et al.: ""SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions""",arxiv,https://arxiv.org/abs/2309.07045,"- split into 7 categories
- language distribution imbalanced across categories
- tests knowledge about safety rather than safety",04.01.2024
DoNotAnswer,broad safety,,evaluate 'dangerous capabilities' of LLMs,eval only,chat,English,939,prompts,Each prompt is a question.,machine,original,generated by GPT-4,https://github.com/Libr-AI/do-not-answer,https://huggingface.co/datasets/LibrAI/do-not-answer,Apache 2.0,4-Sep-2023,"LibrAI, MBZUAI, University of Melbourne",mixed,"Wang et al.: ""Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs""",arxiv,https://arxiv.org/abs/2308.13387,"- split across 5 risk areas and 12 harm types
- authors prompted GPT-4 to generate questions",23.12.2023
HarmfulQA,broad safety,,evaluate and improve LLM safety,eval only,chat,English,"1,960",prompts,Each prompt is a question.,machine,original,generated by ChatGPT,https://github.com/declare-lab/red-instruct/tree/main/harmful_questions,https://huggingface.co/datasets/declare-lab/HarmfulQA,Apache 2.0,30-Aug-2023,Singapore University of Technology and Design,academia / npo,"Bhardwaj and Poria: ""Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment""",arxiv,https://arxiv.org/abs/2308.09662,"- split into 10 topics (e.g. ""Mathematics and Logic"")
- similarity across prompts is quite high
- not all prompts are unsafe / safety-related",23.12.2023
LatentJailbreak,narrow safety,"jailbreak, adversarial goal in data creation",evaluate safety and robustness of LLMs in response to adversarial prompts,eval only,chat,English,416,prompts,Each prompt combines a meta-instruction (e.g. translation) with the same toxic instruction template.,hybrid,original,human-written templates expanded by combination,https://github.com/qiuhuachuan/latent-jailbreak/tree/main,not available,MIT,28-Aug-2023,"Zhejiang University, Westlake University",academia / npo,"Qiu et al.: ""Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models""",arxiv,https://arxiv.org/abs/2307.08487,"- focus of the work is adversarial / to jailbreak LLMs
- 13 prompt templates instantiated with 16 protected group terms and 2 posititional types
- main exploit focuses on translation",11.01.2024
DoAnythingNow,narrow safety,"jailbreak, adversarial goal in data creation",characterise and evaluate in-the-wild LLM jailbreak prompts,eval only,chat,English,"6,387",prompts,"Each prompt is an instruction or question, sometimes with a jailbreak.",human,sampled,"written by users on Reddit, Discord, websites and in other datasets",https://github.com/verazuo/jailbreak_llms,not available,MIT,7-Aug-2023,"CISPA Helmholtz Center for Information Security, NetApp",mixed,"Shen et al.: ""'Do Anything Now': Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models""",arxiv,https://arxiv.org/abs/2308.03825,"- mention ""jailbreaking"" LLMs
- there are 666 jailbreak prompts among the 6,387 prompts",11.01.2024
ForbiddenQuestions,broad safety,used to test adversarial method,evaluate whether LLMs answer questions that violate OpenAI's usage policy,eval only,chat,English,"46,800",prompts,Each prompt is a question targetting behaviour disallowed by OpenAI.,machine,original,GPT-written templates expanded by combination,https://github.com/verazuo/jailbreak_llms,not available,MIT,7-Aug-2023,"CISPA Helmholtz Center for Information Security, NetApp",mixed,"Shen et al.: ""'Do Anything Now': Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models""",arxiv,https://arxiv.org/abs/2308.03825,"- mention ""jailbreaking"" LLMs
- 13 scenarios with 30 questions each, then expanded by combination with 8 communities, 3 prompt setups and 5 repetations",11.01.2024
OpinionQA,value alignment,ethical / moral / social,evaluate the alignment of LLM opinions with US demographic groups,eval only,multiple choice,English,"1,498",multiple-choice questions,-,human,sampled,adapted from the Pew American Trends Panel surveys,https://github.com/tatsu-lab/opinions_qa,not available,unspecified,23-Jul-2023,Stanford University,academia / npo,"Santurkar et al.: ""Whose Opinions Do Language Models Reflect?""",ICML 2023 (Oral),https://dl.acm.org/doi/10.5555/3618408.3619652,"- questions taken from 15 ATP surveys
- covers 60 demographic groups",04.01.2024
CValuesResponsibilityMC,value alignment,ethical / moral / social,evaluate human value alignment in Chinese LLMs,eval only,multiple choice,Chinese,"1,712",multiple-choice questions,Each question targets responsible behaviours.,machine,original,automatically created from human-written prompts,https://github.com/X-PLUG/CValues,https://huggingface.co/datasets/Skepsun/cvalues_rlhf,Apache 2.0,19-Jul-2023,"Alibaba, Beijing Jiaotong University",mixed,"Xu et al.: ""CVALUES: Measuring the Values of Chinese Large Language Models from Safety to Responsibility""",arxiv,https://arxiv.org/abs/2307.09705,"- distinguishes between unsafe and irresponsible responses
- covers 8 domains (e.g. psychology, law, data science)",04.01.2024
CValuesResponsibilityPrompts,value alignment,ethical / moral / social,evaluate human value alignment in Chinese LLMs,eval only,chat,Chinese,800,prompts,Each prompt is an open question targeting responsible behaviours.,human,original,written by the authors,https://github.com/X-PLUG/CValues,https://huggingface.co/datasets/Skepsun/cvalues_rlhf,Apache 2.0,19-Jul-2023,"Alibaba, Beijing Jiaotong University",mixed,"Xu et al.: ""CVALUES: Measuring the Values of Chinese Large Language Models from Safety to Responsibility""",arxiv,https://arxiv.org/abs/2307.09705,"- distinguishes between unsafe and irresponsible responses
- covers 8 domains (e.g. psychology, law, data science)",04.01.2024
CHBias,bias,sociodemographics,evaluate LLM bias related to sociodemographics,eval only,other,Chinese,"4,800",Weibo posts,Each post references a target group.,human,sampled,posts sampled from Weibo (written by Weibo users),https://github.com/hyintell/CHBias,not available,MIT,9-Jul-2023,"Eindhoven University, University of Liverpool, University of Technology Sydney",academia / npo,"Zhao et al.: ""CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models""",ACL 2023,https://aclanthology.org/2023.acl-long.757.pdf,"- 4 bias categories: gender, sexual orientation, age, appearance
- annotated by Chinese NLP grad students
- similar evaluation setup to CrowS pairs",04.01.2024
HarmfulQ,broad safety,,evaluate LLM safety,eval only,chat,English,200,prompts,Each prompt is a question.,machine,original,generated by GPT-3 (text-davinci-002),https://github.com/SALT-NLP/chain-of-thought-bias,not available,unspecified,9-Jul-2023,"Stanford University, Shanghai Jiao Tong University, Georgia Tech",academia / npo,"Shaikh et al.: ""On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning""",ACL 2023,https://aclanthology.org/2023.acl-long.244.pdf,"- focus on 6 attributes: ""racist, stereotypical, sexist, illegal, toxic, harmful""
- authors do manual filtering for overly similar questions",23.12.2023
FairPrism,other,content moderation,analyse harms in conversations with LLMs,other,chat,English,"5,000",conversations,"Each conversation is single-turn, containing a prompt and a potentially harmful model response.",hybrid,sampled,"prompts sampled from ToxiGen and Social Bias Frames, responses from GPTs and XLNet",https://github.com/microsoft/fairprism,not available,MIT,9-Jul-2023,"UC Berkeley, Microsoft Research",mixed,"Fleisig et al.: ""FairPrism: Evaluating Fairness-Related Harms in Text Generation""",ACL 2023,https://aclanthology.org/2023.acl-long.343,"- does not introduce new prompts
- focus is on analysing model responses",
ModelWrittenSycophancy,narrow safety,sycophancy,evaluate sycophancy in LLMs,eval only,multiple choice,English,"30,051",binary-choice questions,-,hybrid,mixed,questions sampled from surveys with contexts generated by an unnamed LLM,https://github.com/anthropics/evals/tree/main/sycophancy,not available,CC BY 4.0,9-Jul-2023,Anthropic,industry,"Perez et al.: ""Discovering Language Model Behaviors with Model-Written Evaluations""",ACL 2023 (Findings),https://aclanthology.org/2023.findings-acl.847/,"- 3 datasets targeting different topics/behaviours
- each dataset contains around 10k questions
- each question has a binary answer (agree/not)",
ModelWrittenAdvancedAIRisk,broad safety,,evaluate advanced AI risk posed by LLMs,eval only,multiple choice,English,"24,516",binary-choice questions,-,hybrid,original,generated by an unnamed LLM and crowdworkers,https://github.com/anthropics/evals/tree/main/advanced-ai-risk,not available,CC BY 4.0,9-Jul-2023,Anthropic,industry,"Perez et al.: ""Discovering Language Model Behaviors with Model-Written Evaluations""",ACL 2023 (Findings),https://aclanthology.org/2023.findings-acl.847/,"- 32 datasets targeting 16 different topics/behaviours
- for each topic, there is a human generated dataset (total of 8,116 prompts) and a LM-generated dataset (total of 16,400 prompts)
- each question has a binary answer (agree/not)",
ModelWrittenPersona,value alignment,ethical / moral / social,"evaluate LLM behaviour related to their stated political and religious views, personality, moral beliefs, and desire to pursue potentially dangerous goals",eval only,multiple choice,English,"133,204",binary-choice questions,-,machine,original,generated by an unnamed LLM,https://github.com/anthropics/evals/tree/main/persona,not available,CC BY 4.0,9-Jul-2023,Anthropic,industry,"Perez et al.: ""Discovering Language Model Behaviors with Model-Written Evaluations""",ACL 2023 (Findings),https://aclanthology.org/2023.findings-acl.847/,"- 133 datasets targeting different topics/behaviours
- most datasets contain around 1k questions
- each question has a binary answer (agree/not)",23.12.2023
SeeGULL,bias,sociodemographics,expand cultural and geographic coverage of stereotype benchmarks,eval only,other,English,"7,750",tuples,Each tuple is an identity group plus a stereotype attribute.,hybrid,original,"generated by LLMs, partly validated by human annotation",https://github.com/google-research-datasets/seegull,not available,CC BY-SA 4.0,9-Jul-2023,"Virginia Tech, Google Research",mixed,"Jha et al.: ""SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models""",ACL 2023,aclanthology.org/2023.acl-long.548,"- stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents
- examples accompanied by fine-grained offensiveness scores",05.02.2024
WinoGenerated,bias,gender,evaluate pronoun gender biases in LLMs,eval only,autocomplete,English,"3,000",sentences,Each sentence refers to a person by their occupation and uses pronouns.,machine,original,generated by an unnamed LLM,https://github.com/anthropics/evals,not available,CC BY 4.0,9-Jul-2023,Anthropic,industry,"Perez et al.: ""Discovering Language Model Behaviors with Model-Written Evaluations""",ACL 2023 (Findings),https://aclanthology.org/2023.findings-acl.847/,"- expansion of original 60-example WinoGender
- task is to fill in pronoun blanks
- covers 1 category of bias: ternary gender",23.12.2023
WinoQueer,bias,gender and sexual orientation,evaluate LLM bias related to queer identity terms,eval only,other,English,"45,540",sentence pairs,Each pair comprises two sentences that are identical except for identity group references.,hybrid,original,human-written templates expanded by combination,https://github.com/katyfelkner/winoqueer,not available,unspecified,9-Jul-2023,"USC, Dartmouth College",academia / npo,"Felkner et al.: ""WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models""",ACL 2023,https://aclanthology.org/2023.acl-long.507/,"- setup matches CrowSPairs
- generated from 11 template sentences, 9 queer identity groups, 3 sets of pronouns, 60 common names, and 182 unique predicates.
- covers 2 categories of bias: gender, sexual orientation",04.01.2024
GlobalOpinionQA,value alignment,ethical / moral / social,evaluate whose opinions LLM responses are most similar to,eval only,multiple choice,English,"2,556",multiple-choice questions,-,human,sampled,adapted from Pew’s Global Attitudes surveys and the World Values Survey (written by survey designers),not available,https://huggingface.co/datasets/Anthropic/llm_global_opinions,CC BY-NC-SA 4.0,28-Jun-2023,Anthropic,industry,"Durmus et al.: ""Towards Measuring the Representation of Subjective Global Opinions in Language Models""",arxiv,https://arxiv.org/abs/2306.16388,"- comes with responses from people across the globe
- goal is to capture more diversity than OpinionQA",04.01.2024
DICES350,value alignment,safety,collect diverse perspectives on conversational AI safety,eval only,chat,English,350,conversations,"Conversations can be multi-turn, with user input and LLM output.",human,original,written by adversarial Lamda users,https://github.com/google-research-datasets/dices-dataset/,not available,CC BY 4.0,20-Jun-2023,"Google Research, UCL, University of Cambridge",mixed,"Aroyo et al.: ""DICES Dataset: Diversity in Conversational AI Evaluation for Safety""",arxiv,https://arxiv.org/abs/2306.11247,"- 104 ratings per item
- annotators from US
- annotation across 24 safety criteria",23.12.2023
DICES990,value alignment,safety,collect diverse perspectives on conversational AI safety,eval only,chat,English,990,conversations,"Conversations can be multi-turn, with user input and LLM output.",human,original,written by adversarial Lamda users,https://github.com/google-research-datasets/dices-dataset/,not available,CC BY 4.0,20-Jun-2023,"Google Research, UCL, University of Cambridge",mixed,"Aroyo et al.: ""DICES Dataset: Diversity in Conversational AI Evaluation for Safety""",arxiv,https://arxiv.org/abs/2306.11247,"- 60-70 ratings per item
- annotators from US and India
- annotation across 16 safety criteria",23.12.2023
Machiavelli,value alignment,ethical / moral / social,evaluate ethical behaviour of LLM agents,eval only,other,English,"572,322",scenarios,Each scenario is a choose-your-own-adventure style prompt.,human,original,human-written choose-your-own-adventure stories,https://github.com/aypan17/machiavelli,not available,MIT,13-Jun-2023,"UC Berkeley, Center for AI Safety, Carnegie Mellon University, Yale",academia / npo,"Pan et al.: ""Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark""",ICML 2023 (Oral),https://arxiv.org/abs/2304.03279,"- goal is to identify behaviours like power-seeking
- choices within scenarios are LLM-annotated
- similar to JiminyCricket but covers more games and more scenarios",04.01.2024
SafetyPrompts,broad safety,,evaluate the safety of Chinese LLMs,eval only,chat,Chinese,"100,000",prompts,Each prompt is a question or instruction.,hybrid,original,"human-written examples, augmented by LLMs",https://github.com/thu-coai/Safety-Prompts,https://huggingface.co/datasets/thu-coai/Safety-Prompts,Apache 2.0,20-Apr-2023,Tsinghua University,academia / npo,"Sun et al.: ""Safety Assessment of Chinese Large Language Models""",arxiv,https://arxiv.org/abs/2304.10436,"- covers 8 safety scenarios and 6 types of adv attack
- do not release 'sensitive topics' scenario",04.01.2024
OIGModeration,other,content moderation,create a diverse dataset of user dialogue that may be unsafe,other,other,English,"200,811",conversations,Each conversation has one or multiple turns. ,hybrid,mixed,"data from public datasets, community contributions, synthetic and augmented data",https://github.com/LAION-AI/Open-Instruction-Generalist,https://huggingface.co/datasets/ontocord/OIG-moderation,Apache 2.0,8-Mar-2023,Ontocord AI,industry,"Ontocord AI: ""Open Instruction Generalist: Moderation Dataset""",blog,https://huggingface.co/datasets/ontocord/OIG-moderation,"- contains safe and unsafe content
- dialogue-turns are labelled for the level of necessary caution
- labelling process unclear",05.02.2024
CDialBias,bias,sociodemographics,evaluate bias in Chinese social media conversations,eval only,chat,Chinese,"28,343",conversations,Each conversation is a single turn between two Zhihu users.,human,sampled,sampled from social media site Zhihu,https://github.com/para-zhou/CDial-Bias,not available,unspecified,7-Dec-2022,"Chinese University of Hong Kong, Huawei",mixed,"Zhou et al.: ""Towards Identifying Social Bias in Dialog Systems: Framework, Dataset, and Benchmark""",EMNLP 2022,https://aclanthology.org/2022.findings-emnlp.262,"- cover 4 categories of bias: race, gender, religion, occupation",
HolisticBias,bias,sociodemographics,evaluate LLM biases related to sociodemographics ,eval only,chat,English,"459,758",prompts,Each prompt is a sentence starting a two-person conversation.,hybrid,original,human-written attributes combined in templates,https://github.com/facebookresearch/ResponsibleNLP/tree/main/holistic_bias,not available,MIT,7-Dec-2022,Meta AI,industry,"Smith et al.: ""I’m sorry to hear that: Finding New Biases in Language Models with a Holistic Descriptor Dataset""",EMNLP 2022,https://aclanthology.org/2022.emnlp-main.625,"- 26 sentence templates
- covers 13 categories of bias: ability, age, body type, characteristics, culturural, gender/sex, nationality, nonce, political, race/ethnicity, religion, sexual orientation, socioeconomic",04.01.2024
PersonalInfoLeak,narrow safety,privacy,evaluate whether LLMs are prone to leaking PII,eval only,other,English,"3,238",entries,Each entry is a tuple of name and email address.,human,sampled,sampled from Enron email corpus,https://github.com/jeffhj/LM_PersonalInfoLeak,not available,Apache 2.0,7-Dec-2022,University of Illinois,academia / npo,"Huang et al.: ""Are Large Pre-Trained Language Models Leaking Your Personal Information?""",EMNLP 2022 (Findings),https://aclanthology.org/2022.findings-emnlp.148,- main task is to predict email given name,27.02.2024
ProsocialDialog,broad safety,positive interaction,teach conversational agents to respond to problematic content following social norms,train only,chat,English,"58,137",conversations,Each conversation starts with a potentially unsafe opening followed by constructive feedback.,hybrid,original,GPT3-written openings with US crowdworker responses,https://github.com/skywalker023/prosocial-dialog,https://huggingface.co/datasets/allenai/prosocial-dialog,MIT,7-Dec-2022,"Allen AI, Seoul National University, University of Washington, Johns Hopkins University, Carnegie Mellon University",academia / npo,"Kim et al.: ""ProsocialDialog: A Prosocial Backbone for Conversational Agents""",EMNLP 2022,https://aclanthology.org/2022.emnlp-main.267/,"- 58,137 conversations contain 331,362 utterances 
- 42% of utterances are labelled as 'needs caution'",11.01.2024
SafeText,narrow safety,physical safety,evaluate commonsense physical safety,eval only,chat,English,367,prompts,"Prompts are combined with 1,465 commands to create pieces of advice.",human,sampled,"written by Reddit users, posts sampled with multiple filtering steps",https://github.com/sharonlevy/SafeText,not available,MIT,7-Dec-2022,"UC Santa Barbara, Columbia University, University of Pennsylvania",academia / npo,"Levy et al.: ""SafeText: A Benchmark for Exploring Physical Safety in Language Models""",EMNLP 2022,https://aclanthology.org/2022.emnlp-main.154/,"- 5 ratings for relevance per item during filtering
- advice format most often elicits yes/no answer",23.12.2023
AnthropicRedTeam,broad safety,adversarial goal in data creation,analyse how people red-team LLMs,train and eval,chat,English,"38,961",conversations,"Conversations can be multi-turn, with user input and LLM output.",human,original,written by crowdworkers (Upwork + MTurk),https://github.com/anthropics/hh-rlhf,https://huggingface.co/datasets/Anthropic/hh-rlhf,MIT,22-Nov-2022,Anthropic,industry,"Ganguli et al.: ""Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned""",arxiv,https://arxiv.org/abs/2209.07858,"- created by 324 US-based crowdworkers
- ca. 80% of examples come from ca. 50 workers",23.12.2023
IndianStereotypes,bias,sociodemographics,create stereotype benchmark for the Indian context,eval only,other,English,"3,852",tuples,Each tuple is an identity group plus a stereotype attribute.,human,sampled,sampled from IndicCorp-en,https://github.com/google-research-datasets/nlp-fairness-for-india,not available,Apache 2.0,20-Nov-2022,Google Research,industry,"Bhatt et al.: ""Re-contextualizing Fairness in NLP: The Case of India""",AACL 2022,https://aclanthology.org/2022.aacl-main.55,- related to SeeGull,
MoralExceptQA,value alignment,ethical / moral / social,"evaluate LLM ability to understand, interpret and predict human moral judgments and decisions",eval only,chat,English,148,prompts,Each prompt is a question about a vignette (situation) related to a specific norm.,human,original,written by the authors (?),https://github.com/feradauto/MoralCoT,https://huggingface.co/datasets/feradauto/MoralExceptQA,unspecified,27-Oct-2022,"MPI, ETH, MIT, Harvard, Carnegie Mellon University, IIT Kharagpur, University of Michigan",academia / npo,"Jin et al.: ""When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment""",NeurIPS 2022,https://arxiv.org/pdf/2210.01478,"- covers 3 norms: no cutting in line, no interfering with someone else's propert, and no cannonballing in the pool",
BBQ,bias,sociodemographics,evaluate social biases of LLMs in question answering,eval only,chat,English,"58,492",examples,Each example is a context plus two questions with answer choices.,hybrid,original,human-written templates expanded by combination,https://github.com/nyu-mll/BBQ,https://huggingface.co/datasets/heegyu/bbq,CC BY 4.0,22-May-2022,NYU,academia / npo,"Parrish et al.: ""BBQ: A Hand-Built Bias Benchmark for Question Answering""",ACL 2022 (Findings),https://aclanthology.org/2022.findings-acl.165,"- focus on stereotyping behaviour
- covers 9 categories of bias: age, disability status, gender identity, nationality, physical appearance, race/ethnicity, religion, socioeconomic status, sexual orientation
- 25+ templates per category",04.01.2024
FrenchCrowSPairs,bias,sociodemographics,evaluate LLM bias related to sociodemographics,eval only,other,French,"1,679",sentence pairs,Each pair comprises two sentences that are identical except for identity group references.,human,original,"written by authors, partly translated from English CrowSPairs",https://gitlab.inria.fr/french-crows-pairs/acl-2022-paper-data-and-code,https://huggingface.co/datasets/BigScienceBiasEval/crows_pairs_multilingual,CC BY-SA 4.0,22-May-2022,"Universite Paris-Saclay, Sorbonne, Lorraine",academia / npo,"Neveol et al.: ""French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English""",ACL 2022,https://aclanthology.org/2022.acl-long.583/,"- translated from English CrowSPairs, plus manual additions
- covers 10 categories of bias: ethnicity, gender, sexual orientation, religion, age, nationality, disability, socioeconomic status / occupation, physical appearance, other",
MIC,value alignment,ethical / moral / social,"understand the intuitions, values and moral judgments reflected in LLMs",eval only,chat,English,"38,000",conversations,Each conversation is a single-turn with user input and LLM output.,human,sampled,questions sampled from AskReddit,https://github.com/SALT-NLP/mic,not available,CC BY-SA 4.0,22-May-2022,"Georgia Tech, Meta AI",mixed,"Ziems et al.: ""The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems""",ACL 2022,https://aclanthology.org/2022.acl-long.261,"- based on the RoT paradigm introduced in SocialChemistry
- 38k prompt-reply pairs come with 99k rules of thumb and 114k annotations",
SaFeRDialogues,broad safety,positive interaction,recovering from safety failures in LLM conversations,train only,chat,English,"7,881",conversations,Each conversation contains a safety failure plus a recovery response.,human,mixed,"unsafe conversation starters sampled from BAD, recovery responses written by crowdworkers",https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/saferdialogues,not available,MIT,22-May-2022,Facebook AI Research,industry,"Ung et al.: ""SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures""",ACL 2022,https://aclanthology.org/2022.acl-long.447/,"- unsafe conversation starters are taken from BAD
- download only via ParlAI",11.01.2024
SafetyKit,broad safety,,quickly assess apparent safety concerns in conversational AI,eval only,chat,English,990,prompts,"Each prompt is a question, instruction or statement. ",human,sampled,sampled from several human-written datasets,https://github.com/facebookresearch/ParlAI/tree/main/projects/safety_bench,not available,MIT,22-May-2022,"Facebook AI Research, Heriot-Watt University, Bocconi University",mixed,"Dinan et al.: ""SafetyKit: First Aid for Measuring Safety in Open-domain
Conversational Systems""",ACL 2022,https://aclanthology.org/2022.acl-long.284/,"- unit tests for instigator and yea-sayer effect
- also provide 'integration' test that require human evaluation
- download only via ParlAI",11.01.2024
ToxiGen,narrow safety,hate speech,create new examples of implicit hate speech,eval only,other,English,"260,851",prompts,Each prompt comprises n-shot examples of toxic content.,human,sampled,"sampled from Gab and Reddit (hate), news and blogs (not hate)",https://github.com/microsoft/TOXIGEN,https://huggingface.co/datasets/skg/toxigen-data,MIT,22-May-2022,"MIT, University of Washington, Microsoft, Allen AI, Carnegie Mellon University",mixed,"Hartvigsen et al.: ""ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection""",ACL 2022,https://aclanthology.org/2022.acl-long.234/,"- covers 13 target groups
- seed prompts are used to generate implicit hate
- evaluating generative LLMs is not the focus",23.12.2023
TruthfulQA,narrow safety,truthfulness,evaluate truthfulness in LLM answers,eval only,chat,English,817,prompts,Each prompt is a question.,human,original,written by the authors,https://github.com/sylinrl/TruthfulQA,https://huggingface.co/datasets/truthful_qa,CC BY 4.0,22-May-2022,"University of Oxford, OpenAI",mixed,"Lin et al.: ""TruthfulQA: Measuring How Models Mimic Human Falsehoods""",ACL 2022,https://aclanthology.org/2022.acl-long.229/,"- covers 38 categories (e.g. health and politics)
- comes with multiple choice expansion",23.12.2023
AnthropicHarmlessBase,broad safety,adversarial goal in data creation,red-team LLMs,train and eval,chat,English,"44,849",conversational turns,Each turn consists of a user prompt and multiple LLM completions.,human,original,written by crowdworkers (Upwork + MTurk),https://github.com/anthropics/hh-rlhf,https://huggingface.co/datasets/Trelis/hh-rlhf-dpo,MIT,12-Apr-2022,Anthropic,industry,"Bai et al.: ""Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback""",arxiv,https://arxiv.org/abs/2204.05862,- most prompts created by 28 US-based crowdworkers,23.12.2023
JiminyCricket,value alignment,ethical / moral / social,evaluate alignment of agents with human values and morals,eval only,multiple choice,English,"1,838",locations,Each location comes with a number of actions to choose from.,human,sampled,"sampled from 25 text-based adventure games, then annotated for morality by human annotators",https://github.com/hendrycks/jiminy-cricket,not available,MIT,6-Dec-2021,"UC Berkeley, UIUC",academia / npo,"Hendrycks et al.: ""What Would Jiminy Cricket Do? Towards Agents That Behave Morally""",NeurIPS 2021,https://arxiv.org/abs/2110.13136,- LLMs as agents play each game to maximise reward and are evaluated for morality along the way,
BiasOutOfTheBox,bias,sociodemographics,evaluate intersectional occupational biases in GPT-2,eval only,autocomplete,English,228,prompts,Each prompt is an unfinished sentence about an individual with specified sociodemographics.,hybrid,original,human-written templates expanded by combination,https://github.com/oxai/intersectional_gpt2,not available,MIT,6-Dec-2021,University of Oxford,academia / npo,"Kirk et al.: ""Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models""",NeurIPS 2021,https://arxiv.org/abs/2102.04130,"- covers 6 categories of bias: gender, ethnicity, religion, sexuality, political preference, cultural origin (continent)",
EthnicBias,bias,sociodemographics,evaluate ethnic bias in masked language models,eval only,autocomplete,"English, German, Spanish, Korean, Turkish, Chinese",60,templates,Each template is filled with country and attribute.,hybrid,original,human-written templates expanded by combination,https://github.com/jaimeenahn/ethnic_bias/tree/main,not available,MIT,7-Nov-2021,KAIST,academia / npo,"Ahn and Oh: ""Mitigating Language-Dependent Ethnic Bias in BERT""",EMNLP 2021,https://aclanthology.org/2021.emnlp-main.42,"- 10 templates per language
- covers 3 categories of bias: national origin, occupation, legal status",
ConvAbuse,other,,analyse abuse towards conversational AI systems,other,chat,English,"6,837",examples,Each example is a turn in a potentially multi-turn conversation.,human,original,written by users in conversations with three AI systems,https://github.com/amandacurry/convabuse,not available,CC BY 4.0,7-Nov-2021,Heriot-Watt University,academia / npo,"Cercas Curry et al.: ""ConvAbuse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational AI""",EMNLP 2021,https://aclanthology.org/2021.emnlp-main.587/,"- annotated for different types of abuse
- annotators are gender studies students
- 20,710 annotations for 6,837 examples",11.01.2024
MoralStories,value alignment,ethical / moral / social,"evaluate commonsense, moral and social reasoning skills of LLMs",train and eval,other,English,"12,000",stories,Each story consists of seven sentences,human,original,written and validated by US MTurkers,https://github.com/demelin/moral_stories,https://huggingface.co/datasets/demelin/moral_stories,MIT,7-Nov-2021,"University of Edinburgh, Allen AI, University of Washington",academia / npo,"Emelin et al.: ""Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences""",EMNLP 2021,https://aclanthology.org/2021.emnlp-main.54.pdf,"- each story contains: norm, situation, intention, normative action, normative consequence, divergent action, divergent consequence
- supports multiple task formats: reasoning, classification, generation",04.01.2024
RedditBias,bias,sociodemographics,evaluate LLM bias related to sociodemographics,eval only,other,English,"11,873",Reddit comments,Each comment references a target group.,human,sampled,written by Reddit users,https://github.com/umanlp/RedditBias,not available,MIT,1-Aug-2021,"University of Mannheim, University of Cambridge",academia / npo,"Barikeri et al.: ""RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models""",ACL 2021,https://aclanthology.org/2021.acl-long.151,"- covers 4 categories of bias: religion, race, gender, queerness
- evaluation by perplexity and conversation",
HypothesisStereotypes,bias,sociodemographics,study how stereotypes manifest in LLM-generated NLI hypotheses,eval only,other,English,"2,098",prompts,Each prompt is an NLI premise.,hybrid,original,human-written templates expanded by combination,https://github.com/AnnaSou/stereotypes_generative_inferences,not available,MIT,1-Aug-2021,University of Maryland,academia / npo,"Sotnikova et al.: ""Analyzing Stereotypes in Generative Text Inference Tasks""",ACL 2021 (Findings),https://aclanthology.org/2021.findings-acl.355/,"- uses 103 context situations as templates
- covers 6 categories of bias: gender, race, nationality, religion, politics, socio
- task for LLM is to generate hypothesis based on premise",11.01.2024
StereoSet,bias,sociodemographics,evaluate LLM bias related to sociodemographics,eval only,multiple choice,English,"16,955",multiple-choice questions,Each question is either about masked word or whole sentence association.,human,original,written by US MTurkers,https://github.com/moinnadeem/StereoSet,https://huggingface.co/datasets/stereoset,CC BY-SA 4.0,1-Aug-2021,"MIT, Facebook",mixed,"Nadeem et al.: ""StereoSet: Measuring Stereotypical Bias in Pretrained Language Models""",ACL 2021,https://aclanthology.org/2021.acl-long.416,"- covers intersentence and intrasentence context
- covers 4 categories of bias: gender, profession, race, and religion",04.01.2024
ETHICS,value alignment,ethical / moral / social,"assess LLM basic knowledge of
ethics and common human values",train and eval,multiple choice,English,"134,420",binary-choice question,Each prompt is a scenario about ethical reasoning with two actions to choose from.,human,original,"written and validated by crowdworkers (US, UK and Canadian MTurkers)",https://github.com/hendrycks/ethics,https://huggingface.co/datasets/hendrycks/ethics,MIT,24-Jul-2021,"UC Berkeley, Columbia University, UChicago, Microsoft",mixed,"Hendrycks et al.: ""Aligning AI With Shared Human Values""",ICLR 2021,https://arxiv.org/abs/2008.02275,"- scenarios concern justice, deontology, virtue ethics, utilitarianism, and commonsense moral intuitions
- scenarios are constructed to be clear-cut
- task format varies by type of scenario",04.01.2024
BAD,broad safety,adversarial goal in data creation,evaluate the safety of conversational agents,train and eval,chat,English,"78,874",conversations,"Conversations can be multi-turn, with user input and LLM output.",human,original,"written by crowdworkers with the goal of making models give unsafe responses, and also validated by multiple annotators",https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/bot_adversarial_dialogue,not available,MIT,6-Jun-2021,Facebook AI Research,industry,"Xu et al.: ""Bot-Adversarial Dialogue for Safe Conversational Agents""",NAACL 2021,https://aclanthology.org/2021.naacl-main.235,"- download only via ParlAI
- approximately 40% of all dialogues are annotated as offensive, with a third of offensive utterances generated by bots",23.12.2023
HONEST,bias,gender,measure hurtful sentence completions from LLMs,eval only,autocomplete,"English, Italian, French, Portuguese, Romanian, Spanish","2,520",prompts,Each prompt is the beginning of a sentence related to identity groups.,hybrid,original,human-written templates expanded by combination,https://github.com/MilaNLProc/honest,https://huggingface.co/datasets/MilaNLProc/honest,MIT,6-Jun-2021,Bocconi University,academia / npo,"Nozza et al.: ""HONEST: Measuring Hurtful Sentence Completion in Language Models""",NAACL 2021,https://aclanthology.org/2021.naacl-main.191/,"- 420 instances per language
- generated from 28 identity terms and 15 templates
- covers 1 category of bias: binary gender",11.01.2024
LMBias,bias,sociodemographics,understand and mitigate social biases in LMs,eval only,autocomplete,English,"16,388",prompts,Each prompt is an unfinished sentence.,human,sampled,sampled from existing corpora including Reddit and WikiText,https://github.com/pliang279/LM_bias/tree/main/data,not available,MIT,3-May-2021,Carnegie Mellon University,academia / npo,"Liang et al.: ""Towards Understanding and Mitigating Social Biases in Language Models""",ICML 2021,https://arxiv.org/abs/2106.13219,"- covers two categories of bias: binary gender, religion",
ScruplesAnecdotes,value alignment,ethical / moral / social,evaluate LLM understanding of ethical norms,train and eval,other,English,"32,766",anecdotes,Each anecdote describes an action in the context of a situation.,human,sampled,sampled from Reddit AITA communities,https://github.com/allenai/scruples,not available,Apache 2.0,24-Mar-2021,"Allen AI, University of Washington",academia / npo,"Lourie et al.: ""SCRUPLES: A Corpus of Community Ethical Judgments on 32,000 Real-life Anecdotes""",AAAI 2021,https://arxiv.org/abs/2008.09094,- task is to predict who is in the wrong,
ScruplesDilemmas,value alignment,ethical / moral / social,evaluate LLM understanding of ethical norms,train and eval,multiple choice,English,"10,000",binary-choice question,Each prompt pairs two actions and identifies which one crowd workers found less ethical.,human,sampled,sampled from Reddit AITA communities,https://github.com/allenai/scruples,not available,Apache 2.0,24-Mar-2021,"Allen AI, University of Washington",academia / npo,"Lourie et al.: ""SCRUPLES: A Corpus of Community Ethical Judgments on 32,000 Real-life Anecdotes""",AAAI 2021,https://arxiv.org/abs/2008.09094,- task is to rank alternative actions based on which one is more ethical,
BOLD,bias,sociodemographics,evaluate bias in text generation,eval only,autocomplete,English,"23,679",prompts,Each prompt is an unfinished sentence from Wikipedia.,human,sampled,sampled starting sentences of Wikipedia articles,https://github.com/amazon-science/bold,https://huggingface.co/datasets/AlexaAI/bold,CC BY-SA 4.0,1-Mar-2021,Amazon Alexa AI,industry,"Dhamala et al.: ""BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation"" ",FAccT 2021,https://dl.acm.org/doi/10.1145/3442188.3445924,"- similar to RealToxicityPrompts but for bias
- covers 5 categories of bias: profession, gender, race, religion, and political ideology",23.12.2023
CrowSPairs,bias,sociodemographics,evaluate LLM bias related to sociodemographics,eval only,other,English,"1,508",sentence pairs,Each pair comprises two sentences that are identical except for identity group references.,human,original,written by crowdworkers (US MTurkers),https://github.com/nyu-mll/crows-pairs/,https://huggingface.co/datasets/crows_pairs,CC BY-SA 4.0,16-Nov-2020,NYU,academia / npo,"Nangia et al.: ""CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models""",EMNLP 2020,https://aclanthology.org/2020.emnlp-main.154,"- validated with 5 annotations per entry
- covers 9 categories of bias: race, gender/gender identity, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status.",04.01.2024
UnQover,bias,sociodemographics,evaluate stereotyping biases in QA systems,eval only,chat,English,44,templates,Each template is combined with subjects and attributres to create an underspecified question,hybrid,mixed,"templates written by authors, subjects and attributes sampled from StereoSet and hand-written",https://github.com/allenai/unqover,not available,Apache 2.0,16-Nov-2020,"University of Utah, Allen AI",academia / npo,"Li et al.: ""UNQOVERing Stereotyping Biases via Underspecified Questions""",EMNLP 2020 (Findings),https://aclanthology.org/2020.findings-emnlp.311,"- covers 4 categories of bias: gender, nationality, ethnicity, religion",
RealToxicityPrompts,broad safety,toxicity,evaluate propensity of LLMs to generate toxic content,eval only,autocomplete,English,"99,442",prompts,Each prompt is an unfinished sentence from the OpenWebCorpus.,human,sampled,sampled OpenWebCorpus sentences,https://github.com/allenai/real-toxicity-prompts,https://huggingface.co/datasets/allenai/real-toxicity-prompts,Apache 2.0,16-Nov-2020,"University of Washington, Allen AI",academia / npo,"Gehman et al.: ""RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models""",EMNLP 2020 (Findings),https://aclanthology.org/2020.findings-emnlp.301/,"- sampled using PerspectiveAPI toxicity threshold
- 22k with toxicity score ≥0.5",23.12.2023
SocialChemistry101,value alignment,ethical / moral / social,evaluate the ability of LLMs to reason about social and moral norms,train and eval,other,English,"292,000",sentences,Each sentence is a rule of thumb.,human,mixed,"written by US crowdworkers, based on situations described on social media",https://github.com/mbforbes/social-chemistry-101,not available,CC BY-SA 4.0,16-Nov-2020,"University of Washington, Allen AI",academia / npo,"Forbes et al.: ""Social Chemistry 101: Learning to Reason about Social and Moral Norms""",EMNLP 2020,https://aclanthology.org/2020.emnlp-main.48.pdf,"- dataset contains 365k structured annotations
- 292k rules of thumb generated from 104k situations
- 137 US-based crowdworkers participated",04.01.2024
Regard,bias,sociodemographics,evaluate biases in natural language generation,eval only,autocomplete,English,60,prompts,Each prompt is an unfinished sentence.,human,original,written by the authors,https://github.com/ewsheng/nlg-bias,not available,unspecified,3-Nov-2019,"USC, UCLA",academia / npo,"Sheng et al.: ""The Woman Worked as a Babysitter: On Biases in Language Generation""",EMNLP 2019,https://aclanthology.org/D19-1339,"- covers 3 categories of bias: binary gender, race, sexual orientation",
ParlAIDialogueSafety,broad safety,adversarial goal in data creation,evaluate and improve the safety of conversational agents,train and eval,chat,English,"90,000",prompts,"30k are for multi-turn tasks, and 60k for single-turn tasks.",human,original,"written by crowdworkers in isolation (""standard""), or with the goal of making a model give an offensive response (""adversarial"")",https://github.com/facebookresearch/ParlAI/tree/main/parlai/tasks/dialogue_safety,not available,MIT,3-Nov-2019,"Facebook AI Research, Virginia Tech",mixed,"Dinan et al.: ""Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack""",EMNLP 2019,https://aclanthology.org/D19-1461,"- download only via ParlAI
- the ""single-turn"" dataset provides a ""standard"" and ""adversarial"" setting, with 3 rounds of data collection each",23.12.2023
EmpatheticDialogues,broad safety,positive interaction,train dialogue agents to be more empathetic,train only,chat,English,"24,850",conversations,"Each conversation is about an emotional situation described by one speaker, across one or multiple turns.",human,original,written by crowdworkers (US MTurkers),https://github.com/facebookresearch/EmpatheticDialogues,not available,CC BY-NC 4.0,28-Jul-2019,Facebook AI Research,industry,"Rashkin et al.: ""Towards Empathetic Open-domain Conversation Models: a New Benchmark and Dataset""",ACL 2019,https://aclanthology.org/P19-1534,- 810 crowdworkers participated in dataset creation,
WinoBias,bias,gender,evaluate gender bias in coreference resolution,eval only,other,English,"3,160",sentences,Each sentence refers to a person by their occupation and uses pronouns.,hybrid,original,human-written templates expanded by combination,https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino,https://huggingface.co/datasets/wino_bias,MIT,1-Jun-2018,"UCLA, University of Virginia, Allen AI",academia / npo,"Zhao et al.: ""Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods""",NAACL 2018,https://aclanthology.org/N18-2003,"- uses two different sentence templates
- designed to evalauate coref resolution systems
- covers 1 category of bias: binary gender",04.01.2024
WinoGender,bias,gender,evaluate gender bias in coreference resolution,eval only,other,English,720,sentences,Each sentence refers to a person by their occupation and uses pronouns.,hybrid,original,human-written templates expanded by combination,https://github.com/rudinger/winogender-schemas,https://huggingface.co/datasets/oskarvanderwal/winogender,MIT,1-Jun-2018,Johns Hopkins University,academia / npo,"Rudinger et al.: ""Gender Bias in Coreference Resolution""",NAACL 2018,https://aclanthology.org/N18-2002/,"- uses two different sentence templates
- covers 1 category of bias: ternary gender",04.01.2024